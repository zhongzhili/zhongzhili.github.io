<a name=top></a><!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Zhongzhi-Li</title><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><link href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/bash.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js></script><script src=https://cdn.jsdelivr.net/npm/vega@5.17.0></script><script src=https://cdn.jsdelivr.net/npm/vega-lite@4.17.0></script><script src=https://cdn.jsdelivr.net/npm/vega-embed@6.12.2></script><script>hljs.initHighlightingOnLoad();</script><link rel=icon href=https://zhongzhili.github.io/media/personal-logo.png></head><body><div class=wrapper><header class=header><nav class=nav><a href=/ class=nav-logo><img src=/media/personal-logo.png width=50 height=50 alt=Hugo-ht></a><ul class=nav-links><li><a href=/>Home</a></li><li><a href=/en/about/>About</a></li><li><a href=/en/posts/>Posts</a></li><li><a href=/cn/posts/>中文</a></li></ul></nav></header><main class=content role=main><div style=text-align:center><h1>Battery thermal runaway prediction based on integrated algorithm</h1><p>Zhongzhi Li
/ 2023-12-22</p><hr></div><span class=article-toolbar><a href=https://github.com/zhongzhili/zhongzhili.github.io/edit/master/content/en/posts/2023-12-22-Battery%20thermal%20runaway%20prediction%20based%20on%20integrated%20algorithm.md style=font-size:24px;color:#000 target=_blank><i class="fa fa-edit" aria-hidden=true title="Suggest an edit of this page"></i></a></span><aside class=toc>Table of Contents:<nav id=TableOfContents><ol><li><a href=#background>Background</a></li><li><a href=#research-introduction>Research introduction</a></li><li><a href=#feature-engineering>Feature engineering</a></li><li><a href=#integrated-learning-algorithm-based-on-stacking>Integrated learning algorithm based on Stacking</a></li><li><a href=#battery-thermal-runaway-prediction-model-based-on-multi-model-fusion-stacking-ensemble-learning-method>Battery thermal runaway prediction model based on multi-model fusion Stacking ensemble learning method</a></li><li><a href=#test-results>Test Results</a></li></ol></nav></aside><div class="body-text list-text"><div class=reminder></div><h2 id=background>Background<a href=#background class=header-anchor arialabel=Anchor> #</a></h2><p style=text-align:justify>Battery thermal runaway means that the battery generates excessive heat during charging or discharging, causing a sharp rise in temperature, and may even cause an explosion or fire. Therefore, in order to ensure the safety and stability of batteries, big data prediction of battery thermal runaway technology has become crucial. In order to effectively prevent and respond to battery thermal runaway risks, researchers have developed a variety of battery thermal runaway detection technologies. These solutions and methods can be divided into three categories: model-based, signal-based and knowledge-based. Existing fault diagnosis methods have certain limitations, such as difficulty in identifying model parameters, lack of reliable and online-applicable signal processing and feature extraction methods, poor stability and low robustness of single model detection, etc. Furthermore, it is difficult to quickly detect faults in their early stages based on actual operating data.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231222-1.png itemprop=contentUrl><img itemprop=thumbnail src=https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231222-1.png width=600></a></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231222-2.png itemprop=contentUrl><img itemprop=thumbnail src=https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231222-2.png width=600></a></figure><h2 id=research-introduction>Research introduction<a href=#research-introduction class=header-anchor arialabel=Anchor> #</a></h2><p style=text-align:justify>In order to improve the accuracy and reliability of battery thermal runaway detection, we apply stack learning to this field. Existing battery thermal runaway detection methods usually use a single detection model. Due to the complexity of the battery system, a single model is difficult to fully capture the battery. Diverse characteristics of thermal runaway. Stacked learning integrates multiple different types of detection models, such as support vector machine (SVM), random forest (Random Forest), neural network, etc., to combine the advantages of each model and improve detection accuracy and generalization capabilities. In addition, we designed 11 statistical features to characterize the rich information of the original data. At the same time, the stacked learner has strong automatic learning and feature selection capabilities, and can automatically extract the most informative features from the original data and optimize the detection performance.</p><h2 id=feature-engineering>Feature engineering<a href=#feature-engineering class=header-anchor arialabel=Anchor> #</a></h2><p style=text-align:justify>The 11 statistics and their calculation formulas:</p>1.Absolute Mean<p><code>$$\bar{x}=\frac{1}{N} \sum_{i=1}^{N}\left|x_{i}\right|$$</code></p><p>2.Standart Deviation</p><p><code>$$\sigma=\sqrt{\frac{1}{N} \sum_{i=1}^{N}\left(x_{i}-\bar{x}\right)^{2}}$$</code></p><p>3.Skewness</p><p><code>$$\mathrm{Sk}=\frac{1}{N} \sum_{i=1}^{N} \frac{\left(x_{i}-\bar{x}\right)^{3}}{\sigma^{3}}$$</code></p><p>4.Kurtosis</p><p><code>$$\mathrm{K}=\frac{1}{N} \sum_{i=1}^{N} \frac{\left(x_{i}-\bar{x}\right)^{4}}{\sigma^{4}}$$</code></p><p>5.Entropy</p><p><code>$$H(X)=-\sum_{i=1}^{N} P\left(x_{i}\right) \log P\left(x_{i}\right)$$</code></p><p>6.RMS</p><p><code>$$x_{r m s}=\sqrt{\left(\frac{1}{N}\right) \sum_{i=1}^{N}(x)^{2}}$$</code></p><p>7.Peak to Peak</p><p><code>$$x_{p}=\max \text { value }-\min \text { value }$$</code></p><p>8.Crest Factor</p><p><code>$$x_{c r e s t}=\frac{\text { max value }}{\mathrm{x}_{\mathrm{rms}}}$$</code></p><p>9.Clearence Factor</p><p><code>$$x_{\text {clear }}=\frac{x_{p}}{\left(\frac{1}{N} \sum_{i=1}^{N} \sqrt{\left|x_{i}\right|}\right)^{2}}$$</code></p><p>10.Shape Factor</p><p><code>$$\frac{\mathcal{X}_{r m s}}{{\bar{x}}}$$</code></p><p>11.Impulse</p><p><code>$$\frac{\text { max value }}{\bar{x}}$$</code></p><h2 id=integrated-learning-algorithm-based-on-stacking>Integrated learning algorithm based on Stacking<a href=#integrated-learning-algorithm-based-on-stacking class=header-anchor arialabel=Anchor> #</a></h2><p style=text-align:justify>The Stacking integrated learning framework first divides the original data set into several sub-datasets and inputs them into each base learner of the first-layer prediction model. Each base learner outputs its own prediction result. Then, the output of the first layer is used as the input of the second layer to train the meta-learner of the second layer prediction model, and then the model in the second layer outputs the final prediction result. The Stacking learning framework generalizes the output results of multiple models to improve the overall prediction accuracy.</p><h2 id=battery-thermal-runaway-prediction-model-based-on-multi-model-fusion-stacking-ensemble-learning-method>Battery thermal runaway prediction model based on multi-model fusion Stacking ensemble learning method<a href=#battery-thermal-runaway-prediction-model-based-on-multi-model-fusion-stacking-ensemble-learning-method class=header-anchor arialabel=Anchor> #</a></h2><p style=text-align:justify>Based on the prediction ability of the base learner, in the first layer of the Stacking model, in addition to the XGBoost algorithm that is widely used in major competitions, we also selected the random forest (RF) with excellent prediction performance. As a base learner, the model adopts the bagging integrated learning method, has excellent learning ability and rigorous mathematical theory support, and has been widely used in various fields. To sum up, the first layer of the stacking integration model initially selects the base learners as XGBoost and RF, and the second layer selects the K nearest neighbor model as the meta-learner. The model architecture is shown in the figure below.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231222-3.png itemprop=contentUrl><img itemprop=thumbnail src=https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231222-3.png width=700></a></figure><p style=text-align:justify>In addition, the training set of the meta-learner is generated from the output of the base learner. If the training set of the base learner is directly used to generate the secondary training set, serious overfitting may occur. In order to prevent the data from being repeatedly learned by the double-layer learner and avoid the occurrence of the "over-fitting" effect, the data usage process needs to be divided reasonably. According to the two selected base learners, the original training data set needs to be divided into two sub-data sets first, and the data IDs of each piece of data do not overlap each other. For a single base learner, use one data block as the training set, corresponding to The remaining data blocks serve as the test set. Each base learner can output a prediction result for its own test data set, and the two results can eventually be merged into a new data set.</p><h2 id=test-results>Test Results<a href=#test-results class=header-anchor arialabel=Anchor> #</a></h2><p style=text-align:justify>We choose the XGBoost model and the random forest model as comparison models to verify the effect of the stacked model. First, the XGBoost model and the random forest model were used as thermal runaway fault detection algorithms to conduct experiments, and the results are shown in the figures. It can be found that XGBoost and random forest algorithms still have the problem of classification errors for some data. In particular, both of them misclassify a large amount of thermal runaway data as normal data in real situations. The effect of the model still has a lot of room for improvement. We use the XGBoost model and the random forest model as the first layer classifier of the stacked model, and the KNN model as the second layer classifier. The results of training and testing using the same data are shown in the figure. It can be found that the model proposed in this patent effectively improves the problem of low thermal runaway detection accuracy. At the same time, analyzing the results shown in the table, the stacking model proposed in this patent not only improves the accuracy of model testing, but also improves the accuracy of the training process. And the deviation of the accuracy results of multiple trainings is reduced, which shows that this model can effectively improve the accuracy and stability of detection.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231222-4.png itemprop=contentUrl><img itemprop=thumbnail src=https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231222-4.png width=600></a></figure><table><thead><tr><th>Water</th><th>XGBoost</th><th>RF</th><th>Stacking Model</th></tr></thead><tbody><tr><td>Train CV Accuracy</td><td>0.88 (+/- 0.11)</td><td>0.93 (+/- 0.03)</td><td><strong>0.96 (+/- 0.01)</strong></td></tr><tr><td>Train Accuracy</td><td>0.75</td><td>0.94</td><td><strong>0.98</strong></td></tr><tr><td>Test Accuracy</td><td>0.72</td><td>0.92</td><td><strong>0.96</strong></td></tr></tbody></table><p style=color:#777>Last modified on 2023-12-22</p></div><a href=#top><i class="fa fa-chevron-up" style=font-size:30px;color:#000></i></a></main><footer class=footer><script type=text/javascript src=/js/math-code.js></script><script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=text/javascript src=/js/center-img.js></script><ul class=footer-links><li><a href=/en/posts/index.xml type=application/rss+xml title="RSS feed">Subscribe</a></li><li><a href=http://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank>License
<i class="fa fa-cc" aria-hidden=true title="Attribution-NonCommercial-ShareAlike 4.0 International"></i></a></li></ul><div class=copyright-text>©
Zhongzhi Li
2022-2023</div></footer>