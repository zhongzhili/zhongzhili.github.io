---
title: "Reading Notes of Paper"
date: 2023-12-18
author: Zhongzhi Li
slug: first-post
draft: false
toc: true
categories:
  - test
tags:
  - article
  - English
---

{{<block class="reminder">}}

Article [link](https://tidel.mie.utoronto.ca/pubs/tracegen_camera_20210926.pdf) if you are interested.

{{<end>}}

# Request sequence generation of VM
## Article title

Generating Complex, Realistic Cloud Workloads using Recurrent Neural Networks

## Author and organization

Shane Bergsma (Huawei Canada Research Center), Timothy Zeyl (Huawei Canada Research Center), Arik Senderovich (University of Toronto), J. Christopher Beck (University of Toronto)

## Problems solved by the article

<p style="text-align: justify;">To predict the load of virtual machines dynamically created by users on the cloud, a Cloud Workload can be simply regarded as a Trace of VMs created by users. Each record records the creation time of the VM and the number of resources requested by the VM (CPU, Memory, Network, etc.), VM survival time, etc.</p>

<p style="text-align: justify;">If the algorithm can accurately predict how the workload will change in the future, resource allocation can be planned more accurately. The accurate prediction model can also be used to tune the scheduling algorithm, improve resource utilization, etc.</p>

## Part of the difficulty with this problem

<p style="text-align: justify;">One of the difficulties is that historical data cannot be used directly for prediction and optimization:</p>

<p style="text-align: justify;">The amount of historical data is not large enough. For example, a scheduler based on reinforcement learning requires large batches of data;</p>

<p style="text-align: justify;">Historical data has limitations. It contains limited information and cannot be used for long-term load forecasting;</p>

## Limitations of other approaches to the problem

<p style="text-align: justify;">Traditional modeling method:</p>

<p style="text-align: justify;">It is assumed that the time when a user creates a VM follows an independent Poisson distribution;</p>

<p style="text-align: justify;">It is assumed that user demand for resources follows an independent polynomial distribution;</p>

<p style="text-align: justify;">It is assumed that the life cycle of each VM (or Job) is also independent of each other.</p>

<p style="text-align: justify;">Various independence assumptions in traditional modeling methods make the fitting between modeling results and real data low, making it difficult to generate real, high-quality workloads, resulting in inaccurate final decisions.</p>

## The idea of the method proposed in the article

<p style="text-align: justify;">This paper introduces a predictive model for large-scale cloud workloads that captures long-distance inter-job correlations in terms of arrival rates, resource requirements, and lifetimes. The workload is modeled as a three-stage generation process with three separate models: (1) predicting the number of batch arrivals over time (Batch Arrival Modeling), (2) predicting the order in which resources are requested (Resource Modeling), and (3) predicting life cycle (Lifetime Modeling). Figure 1 shows the calculation framework of the algorithm. It takes historical data as input and passes through three models. Each model uses the output of the previous model as the input of the current model. Finally, a Trace is generated, which contains the creation of each VM. time, end time, and request for resources.</p>

{{<figure src="https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231218-1.png" title="Figure 1. Three-stage workload generation process in a cycle" width="260">}}

Next, the three stages will be analyzed in turn:

**Batch Arrival Modeling**

<p style="text-align: justify;">The first model uses Poisson regression to generate the number of batches arriving within a period of time (e.g. 5 min) (a batch is defined as all jobs submitted by the same user within this period of time).</p>

{{<figure src="https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231218-2.png" width="500">}}

Poisson regression model establishment:

<p style="text-align: justify;">Non-homogeneous Poisson regression assumes that the number of events `N_p` occurring in each time period is modeled by a Poisson distribution with a mean of `\mu_{p}`, and its expression is `$\mu_{p}=\exp (w \cdot x_{p})$`, where `x_p` is a feature vector and `w` is a parameter to be optimized vector. Given some training data consisting of an event number vector `y` and a feature vector matrix `X`, fit the parameters by minimizing the negative log-likelihood (NLL) of the training data:
$$
\begin{aligned}
\text{Loss} & =-\log (\operatorname{Pr}(\mathbf{y} \mid \mathbf{X}, \mathbf{w})) \\
& =\sum_{p} \mu_{p}-y_{p} \log \left(\mu_{p}\right)
\end{aligned}
$$



The article adds an elastic network regularization term to this loss function to constrain parameters, reduce model complexity, and reduce the risk of overfitting of the model.

The feature vector matrix X contains three types of time feature information:

HOD: Hour-of-day, from 1 to 24 (one-hot-encoded);

DOW: Day-of-week, from 1 to 7 (one-hot-encoded);

DOH: Day-of-history, from 1 to `ùëÅ` (survival-encoded), where there are `ùëÅ` total days in the history;

**Resource Modeling**

The second model uses LSTM to generate flavors (i.e. requested resource types, such as CPU, Memory) for all jobs within a period of time.

{{<figure src="https://raw.githubusercontent.com/zhongzhili/zhongzhili.github.io/master/content/en/fig/20231218-3.png" width="500">}}

At each calculation step `t`, the Flavor LSTM returns a distribution containing `K` (`K` is 16) possible flavors, and a special end-of-batch (EOB) token to indicate a user's set of job requests over a period of time. The LSTM generates vectors of length `ùêæ+1`, which are fed into the softmax function, which returns in step `t`, the multinomial distribution over the `ùêæ` species label to which the flavor belongs:

`$$\operatorname{Pr}\left(\hat{f}_t=k \mid \mathbf{y}_t\right)=\frac{\exp \left(y_t^k\right)}{\sum_{k^{\prime}=1}^{K+1} \exp \left(y_t^{k^{\prime}}\right)}$$`

The standard method of minimizing the negative log-likelihood of training data (CrossEntropyLoss) is used to optimize the adjustment parameters. The loss value of the article is the cross-entropy loss value plus the value of the regularization penalty term, and then the parameters are optimized through gradient backpropagation.

The main flavor feature at step t is one-hot encoding of the flavor from the previous step `ùë°‚àí1`. If the previous step was the end of the batch, or the first flavor in the sequence is being generated, then pass the EOB token as input.

## Math

For example, 

`$1 + 1 = 3$`

You can add Tags & References:

`$$p(x) = \frac{1}{\sigma \sqrt{2 \pi}} exp \left(-\frac{1}{2}\left[\frac{x-\mu}{\sigma}\right]^2\right)\tag{1.1}\label{eq1.1}$$`

Aligning equations[^3] are also possible, 

`\begin{align}
\sqrt{37} & = \sqrt{\frac{73^2-1}{12^2}} \\
 & = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}} \\ 
 & = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}} \\
 & = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}} \\ 
 & \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right)
\end{align}`